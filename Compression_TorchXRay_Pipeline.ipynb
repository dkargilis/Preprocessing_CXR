{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9ifxbndyJu"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWe0Xy8J6RiZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "import skimage.transform\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "import time\n",
        "import cv2\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "#import tqdmpi\n",
        "from scipy.stats import ttest_rel\n",
        "import pandas as pd\n",
        "import sklearn, sklearn.metrics\n",
        "from skimage.io import imread\n",
        "import torchxrayvision as xrv\n",
        "import scipy\n",
        "from scipy import stats\n",
        "from torch._C import NoneType\n",
        "import statistics\n",
        "import seaborn as sns\n",
        "import copy\n",
        "from operator import add\n",
        "from operator import truediv\n",
        "\n",
        "import os,sys\n",
        "os.environ['OPENCV_IO_ENABLE_JASPER'] = 'True'\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "contentFolder = '/content/drive/MyDrive/UM2ii/'\n",
        "\n",
        "destinationFolder = '/content/'\n",
        "\n",
        "#Default return from XRV models (except for Official Chexpert model)\n",
        "default_pathologies = ['Atelectasis',\n",
        " 'Consolidation',\n",
        " 'Infiltration',\n",
        " 'Pneumothorax',\n",
        " 'Edema',\n",
        " 'Emphysema',\n",
        " 'Fibrosis',\n",
        " 'Effusion',\n",
        " 'Pneumonia',\n",
        " 'Pleural_Thickening',\n",
        " 'Cardiomegaly',\n",
        " 'Nodule',\n",
        " 'Mass',\n",
        " 'Hernia',\n",
        " 'Lung Lesion',\n",
        " 'Fracture',\n",
        " 'Lung Opacity',\n",
        " 'Enlarged Cardiomediastinum']\n",
        "\n",
        "#Pathologies we are using\n",
        "included_pathologies = ['Atelectasis',\n",
        " 'Pneumothorax',\n",
        " 'Edema',\n",
        " 'Effusion',\n",
        " 'Pneumonia',\n",
        " 'Cardiomegaly']\n",
        "\n",
        "#Default return from Official Chexpert model\n",
        "chex_pathologies = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\"]\n",
        "\n",
        "#Preprocessing methods we are using\n",
        "preprocessingMethods = [\n",
        "                        \"ir_bilinear_skimage\",\n",
        "                        \"ir_interarea_cv2\",\n",
        "                        \"ir_bilinear_cv2\",\n",
        "                        \"ir_cubic_cv2\",\n",
        "                        \"ir_lanczos_cv2\",\n",
        "                        \"clahe_true_bilinear_skimage\",\n",
        "                        \"compr_j2k_bilinear_skimage\",\n",
        "                        \"compr_j2k_cr2_bilinear_skimage\",\n",
        "                        \"compr_j2k_cr4_bilinear_skimage\",\n",
        "                        \"compr_j2k_cr8_bilinear_skimage\",\n",
        "                        \"compr_j2k_cr16_bilinear_skimage\",\n",
        "                        \"compr_j2k_cr32_bilinear_skimage\"\n",
        "                        ]\n",
        "\n",
        "\n",
        "#Identify subset of NIH images that are coded as RGB\n",
        "#RGB_indices list is used to exclude these images during analysis\n",
        "files = os.listdir('/content/NIH_cohort_balanced/')\n",
        "files.sort()\n",
        "RGB_indices = []\n",
        "for n in range(len(files)):\n",
        "    #Iterate through sorted files\n",
        "    #Ensure we are only loading image files\n",
        "    if 'png' in files[n]:\n",
        "      Img = Image.open('/content/NIH_cohort_balanced/' + files[n])\n",
        "      if (Img.mode != \"L\"):\n",
        "        RGB_indices.append(n)\n",
        "print(RGB_indices)\n",
        "print(len(RGB_indices))\n",
        "\n",
        "\n",
        "def auc(labels,outputs):\n",
        "   \"\"\"\n",
        "   Finds AUROC given labels and outputs, assumes included_pathologies is defined globally\n",
        "\n",
        "    Args:\n",
        "        labels: list of lists of labels\n",
        "        outputs: list of lists of outputs\n",
        "    Returns:\n",
        "        Prints AUROC for each pathology\n",
        "\n",
        "   \"\"\"\n",
        "   for i in range(6):\n",
        "    if len(np.unique(np.asarray(labels)[:,i])) > 1:\n",
        "        auc = sklearn.metrics.roc_auc_score(np.asarray(labels)[:,i], np.asarray(outputs)[:,i])\n",
        "    else:\n",
        "        auc = \"(Only one class observed)\"\n",
        "    print(included_pathologies[i], auc)\n",
        "\n",
        "\n",
        "\n",
        "def aucChex(labels,outputs):\n",
        "  \"\"\"\n",
        "  Does the same thing as auc() but for the official Chexpert model, utilizing chex_pathologies instead of included_pathologies\n",
        "\n",
        "  Args:\n",
        "      labels: list of lists of labels\n",
        "      outputs: list of lists of outputs\n",
        "  Returns:\n",
        "      None\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(len(chex_pathologies)):\n",
        "    if len(np.unique(np.asarray(labels)[:,i])) > 1:\n",
        "        labelsIndex = None\n",
        "        MatchIdentified = False\n",
        "        #Need to figure out which labels correspond to this chex model output\n",
        "        for p in range(len(included_pathologies)):\n",
        "          if str(included_pathologies[p]) == str(chex_pathologies[i]):\n",
        "            labelsIndex = p\n",
        "            MatchIdentified = True\n",
        "        if MatchIdentified:\n",
        "          auc = sklearn.metrics.roc_auc_score(np.asarray(labels)[:,labelsIndex], np.asarray(outputs)[:,i])\n",
        "          print(chex_pathologies[i], auc)\n",
        "    else:\n",
        "        auc = \"(Only one class observed)\"\n",
        "\n",
        "class customXRayResizer(object):\n",
        "\n",
        "  \"\"\"\n",
        "  Over-writing the default XRV resizer to use alternate methods of resizing\n",
        "  Return resized image based on engine and resizing method input on call\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, size, engine=\"cv2\", interp=\"interarea\"):\n",
        "      self.size = size\n",
        "      self.engine = engine\n",
        "      self.interp = interp\n",
        "  def __call__(self, img):\n",
        "      if(self.engine == 'skimage'):\n",
        "        return skimage.transform.resize(img, (1, self.size, self.size), mode='constant', preserve_range=True).astype(np.float32)\n",
        "      elif (self.engine == 'cv2'):\n",
        "        if (self.interp == 'interarea'):\n",
        "          return cv2.resize(img[0,:,:],\n",
        "                              (self.size, self.size),\n",
        "                              interpolation = cv2.INTER_AREA\n",
        "                            ).reshape(1,self.size,self.size).astype(np.float32)\n",
        "        elif(self.interp == 'bilinear'):\n",
        "          return cv2.resize(img[0,:,:],\n",
        "                              (self.size, self.size),\n",
        "                              interpolation = cv2.INTER_LINEAR\n",
        "                            ).reshape(1,self.size,self.size).astype(np.float32)\n",
        "        elif(self.interp == 'nearest'):\n",
        "          return cv2.resize(img[0,:,:],\n",
        "                              (self.size, self.size),\n",
        "                              interpolation = cv2.INTER_NEAREST\n",
        "                            ).reshape(1,self.size,self.size).astype(np.float32)\n",
        "        elif(self.interp == 'cubic'):\n",
        "          return cv2.resize(img[0,:,:],\n",
        "                              (self.size, self.size),\n",
        "                              interpolation = cv2.INTER_CUBIC\n",
        "                            ).reshape(1,self.size,self.size).astype(np.float32)\n",
        "        elif(self.interp == 'lanczos'):\n",
        "          return cv2.resize(img[0,:,:],\n",
        "                              (self.size, self.size),\n",
        "                              interpolation = cv2.INTER_LANCZOS4\n",
        "                            ).reshape(1,self.size,self.size).astype(np.float32)\n",
        "\n",
        "def apply_model(model, clahe = False, resize = 'bilinear', eng = 'cv2', fileDirectory=\"\", fileNames=[]):\n",
        "\n",
        "  \"\"\"\n",
        "  Applies model to images in fileDirectory using fileNames, returns list of model predictions\n",
        "\n",
        "  Args:\n",
        "      model: model to apply\n",
        "      clahe: boolean, whether to apply CLAHE to images\n",
        "      resize: string, resizing method to use\n",
        "      eng: string, engine to use for resizing\n",
        "      fileDirectory: string, directory containing images to apply model to\n",
        "      fileNames: list of strings, names of files to apply model to\n",
        "  Returns:\n",
        "      modelPredictions: list of lists of model predictions\n",
        "  \"\"\"\n",
        "\n",
        "  modelPredictions = []\n",
        "\n",
        "  model.op_threshs = None\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      #Iterate through filenames and process each image\n",
        "      for i in range(len(fileNames)):\n",
        "\n",
        "          #Read next image from file and apply CLAHE if requested\n",
        "          if(clahe):\n",
        "            #Reading the image from the next image directory\n",
        "            tmpImg = cv2.imread(fileDirectory + fileNames[i])\n",
        "            #Ensure black and white color scheme\n",
        "            image_bw = cv2.cvtColor(tmpImg, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # The declaration of CLAHE\n",
        "            clahe = cv2.createCLAHE()\n",
        "            tmpImg = clahe.apply(image_bw)\n",
        "\n",
        "          else:\n",
        "            #No CLAHE -> read image using default skimage.imread\n",
        "            tmpImg = imread(fileDirectory + fileNames[i])\n",
        "            #Ensure black and white color scheme\n",
        "            #tmpImg = cv2.cvtColor(tmpImg, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "          #Normalize image\n",
        "          tmpImg = xrv.datasets.normalize(tmpImg, 255)\n",
        "\n",
        "          # Check that images are 2D arrays\n",
        "          if len(tmpImg.shape) > 2:\n",
        "              tmpImg = tmpImg[:, :, 0]\n",
        "          if len(tmpImg.shape) < 2:\n",
        "              print(\"error, dimension lower than 2 for image\")\n",
        "\n",
        "          # Add color channel\n",
        "          tmpImg = tmpImg[None, :, :]\n",
        "\n",
        "          #RESIZE\n",
        "          #Get custom resizer indicating engine and resizing method\n",
        "          transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
        "                                                  customXRayResizer(size=224, engine=eng, interp=resize)])\n",
        "\n",
        "          #Apply resizing\n",
        "          tmpImg = transform(tmpImg)\n",
        "\n",
        "          #Get model predictions\n",
        "          currentPrediction = model(torch.from_numpy(tmpImg).to(device).unsqueeze(0))\n",
        "          modelPredictions.append(currentPrediction.detach().cpu().numpy()[0])\n",
        "          tmpImg = torch.from_numpy(tmpImg).to(device).unsqueeze(0)\n",
        "          #print(i)\n",
        "          if i % 100 == 0:\n",
        "            print(f\"{i} images done\")\n",
        "      return modelPredictions\n",
        "\n",
        "def organizeImages(fileDir=\"\", labelsDir=\"\", fileExtension=\"\"):\n",
        "\n",
        "  \"\"\"\n",
        "  Organizes images in fileDir and labels in labelsDir into two lists, one containing image names and one containing labels\n",
        "  These are returned in the same case order\n",
        "\n",
        "  Args:\n",
        "      fileDir: string, directory containing images\n",
        "      labelsDir: string, directory containing labels\n",
        "      fileExtension: string, file extension of images\n",
        "  Returns:\n",
        "      imgNames: list of strings, names of images in fileDir\n",
        "      labels: list of strings, labels of images in fileDir in same order as imgNames\n",
        "  \"\"\"\n",
        "#Return two variables; containing image names and disease labels in the same order\n",
        "\n",
        "  #Read labels file\n",
        "  labcsv = pd.read_csv(labelsDir)\n",
        "  #List of image names\n",
        "  series = labcsv['Image']\n",
        "\n",
        "  updatedNames = []\n",
        "\n",
        "  #Add each image file to list of names without file extension\n",
        "  for x in range(len(series)):\n",
        "    if(series[x].endswith('.png') or series[x].endswith('.jpg')):\n",
        "      updatedNames.append(series[x][:-4]+fileExtension)\n",
        "\n",
        "  #Convert to series\n",
        "  updatedNames = pd.Series(updatedNames)\n",
        "\n",
        "  #List files\n",
        "  files = os.listdir(fileDir)\n",
        "  files.sort()\n",
        "\n",
        "  #Create new lists that will contain sorted labels and image names\n",
        "  labels = []\n",
        "  imgNames = []\n",
        "  #For each image, identify the corresponding line in the labels file.\n",
        "  #Add the corresponding label to labels[]\n",
        "\n",
        "  for file in files:\n",
        "    #Iterate through sorted files\n",
        "    #Ensure we are only loading image files\n",
        "    if fileExtension in file:\n",
        "\n",
        "      #Get index of current file in labels series\n",
        "      index = updatedNames[updatedNames == file].index[0]\n",
        "      #Get disease labels data for current file based on index\n",
        "      nextLab = labcsv.iloc[index].to_numpy()[1:]\n",
        "\n",
        "      #Add current file labels to ordered list of labels\n",
        "      labels.append(nextLab)\n",
        "      #Add current file name to ordered list of image names\n",
        "      imgNames.append(file)\n",
        "\n",
        "  #Convert to np array\n",
        "  labels = np.array(labels, dtype=np.float32)\n",
        "\n",
        "  return imgNames,labels\n",
        "\n",
        "def generateTable(pathologyIndex=0,sm=[]):\n",
        "\n",
        "  \"\"\"\n",
        "  Generates table of values for a given pathology index\n",
        "\n",
        "  Args:\n",
        "      pathologyIndex: int, index of pathology to generate table for\n",
        "      sm: list of lists of model predictions\n",
        "  Returns:\n",
        "      vals: list of floats, values for pathologyIndex\n",
        "  \"\"\"\n",
        "  #Reformat data to generate table\n",
        "  vals = []\n",
        "  for i in range(len(sm)):\n",
        "    vals.append(sm[i][pathologyIndex])\n",
        "  return vals\n",
        "\n",
        "def pickleVar(path=\"\", var=\"\"):\n",
        "  \"\"\"\n",
        "  Pickle/Save a variable as a file\n",
        "\n",
        "  Args:\n",
        "      path: string, path to save file to\n",
        "      var: variable to save\n",
        "  Returns:\n",
        "      None\n",
        "\n",
        "  \"\"\"\n",
        "  with open(path, \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(var, fp)\n",
        "\n",
        "def getImageDimensions(dir):\n",
        "  \"\"\"\n",
        "  Iterates through images in a directory and prints metrics of image dimensions\n",
        "\n",
        "  Args:\n",
        "      dir: string, directory to iterate through\n",
        "  Returns:\n",
        "      None\n",
        "  \"\"\"\n",
        "\n",
        "  #Prints metrics of image dimensions for a given directory\n",
        "  parentDir = dir\n",
        "\n",
        "  widths = []\n",
        "  heights = []\n",
        "  areas = []\n",
        "  size = []\n",
        "  bitmaps = []\n",
        "\n",
        "  files = os.listdir(parentDir)\n",
        "  files.sort()\n",
        "\n",
        "  #Iterate through files and collect stats\n",
        "  for x in range(len(files)):\n",
        "      #Ensure we are only examining image files\n",
        "      if '.jpg' in str(files[x]) or '.png' in str(files[x]) or '.j2k' in str(files[x]):\n",
        "          currentPath = parentDir + files[x]\n",
        "          file_stats = os.stat(currentPath)\n",
        "          size.append(file_stats.st_size)\n",
        "\n",
        "          img = Image.open(currentPath)\n",
        "          widths.append(img.width)\n",
        "          heights.append(img.height)\n",
        "          areas.append(img.width*img.height)\n",
        "          bitmaps.append(img.width*img.height*8)\n",
        "\n",
        "\n",
        "  print(\"Num images:\")\n",
        "  print(len(widths))\n",
        "\n",
        "  print(\"Avg Uncompressed Bitmap Kb\")\n",
        "  print((np.sum(bitmaps)/len(bitmaps))/(8000) )\n",
        "  print(f\"({str(np.std( np.divide(bitmaps, 8000) ) )})\")\n",
        "\n",
        "  print(\"Sum Uncompressed Bitmap Kb\")\n",
        "  print((np.sum(bitmaps))/1000 )\n",
        "\n",
        "  print(\"AVERAGE (STDEV) WIDTH:\")\n",
        "  print(np.sum(widths)/len(widths))\n",
        "  print(f\"({str(np.std(widths))})\")\n",
        "\n",
        "  print(\"Width Min, Max\")\n",
        "  print(min(widths))\n",
        "  print(max(widths))\n",
        "\n",
        "  print(\"AVERAGE (STDEV) HEIGHT:\")\n",
        "  print(np.sum(heights)/len(heights))\n",
        "  print(f\"({str(np.std(heights))})\")\n",
        "\n",
        "  print(\"Height Min, Max\")\n",
        "  print(min(heights))\n",
        "  print(max(heights))\n",
        "\n",
        "  print(\"AVERAGE (STDEV) AREA:\")\n",
        "  print(np.sum(areas)/len(areas))\n",
        "  print(f\"({str(np.std(areas))})\")\n",
        "\n",
        "  print(\"Area Min, Max\")\n",
        "  print(min(areas))\n",
        "  print(max(areas))\n",
        "\n",
        "  print(\"Sum size, Kb\")\n",
        "  print((np.sum(size))/1000 )\n",
        "\n",
        "  print(\"AVERAGE (STDEV) SIZE bytes:\")\n",
        "  print(np.sum(size)/len(size))\n",
        "  print(f\"({str(np.std(size))})\")\n",
        "\n",
        "  print(\"AVERAGE (STDEV) SIZE IN MB\")\n",
        "  print((np.sum(size)/len(size))/1000000)\n",
        "  print(f\"({str(np.std(np.divide(size, 1000000)))})\")\n",
        "\n",
        "def loadSavedVariables(pathologies, model, dataset, preprocessingMethods):\n",
        "  \"\"\"\n",
        "  Loads saved variables (previously pickled predictions) based on a model, dataset, and preprocessing methods.\n",
        "  Assumes variables are saved in the AutoSavedVariables folder under naming conventions of model_dataset_preprocessingMethod\n",
        "\n",
        "  Args:\n",
        "      pathologies: list of strings, list of pathologies to load variables for\n",
        "      model: string, model to load variables for\n",
        "      dataset: string, dataset to load variables for\n",
        "      preprocessingMethods: list of strings, list of preprocessing methods to load variables for\n",
        "\n",
        "  Returns:\n",
        "      results_list: list of lists of floats, list of raw predictions for each preprocessing method\n",
        "      frames: dictionary of dataframes, dictionary of dataframes of predictions organized by pathology\n",
        "  \"\"\"\n",
        "\n",
        "  #Load saved variables based on a model, dataset, and preprocessing methods\n",
        "  #Returns list of raw predictions and frame reorganized by disease label for AUC calculation\n",
        "  results_list = []\n",
        "\n",
        "  #For each preprocessing method, attempt to load the saved model\n",
        "  for t in preprocessingMethods:\n",
        "    currentPath = contentFolder + \"AutoSavedVariables/\" + model + \"_\" + dataset + \"_\" + t\n",
        "    if os.path.isfile(currentPath):\n",
        "      with open(currentPath, \"rb\") as fp:   # Unpickling\n",
        "        currentVar = pickle.load(fp)\n",
        "      results_list.append(currentVar)\n",
        "    else:\n",
        "      print(currentPath + \" does not exist\")\n",
        "\n",
        "  frames = {}\n",
        "  for x in range(len(pathologies)):\n",
        "    df = pd.DataFrame()\n",
        "    for y in range(len(results_list)):\n",
        "      df[str(y)] = pd.Series(generateTable(x, results_list[y]))\n",
        "    frames[pathologies[x]] = df\n",
        "\n",
        "  return results_list, frames\n",
        "\n",
        "\n",
        "\n",
        "def get_base_comparison_labels_and_preds(conditionIndex, variables_jpg, base, basename,\n",
        "                                 comparison, comparisonname, isChexModel):\n",
        "\n",
        "  \"\"\"\n",
        "  Minor function to pull specific prediction data, creates a dataframe based on loaded data, a given pathology index, and base and comparison groups (two types of preprocessing).\n",
        "\n",
        "\n",
        "  Args:\n",
        "    conditionIndex: index of pathology in variables_jpg\n",
        "    variables_jpg: loaded prediction outputs from the previous results\n",
        "    base: list of predictions from base preprocessing\n",
        "    basename: name of base preprocessing\n",
        "    comparison: list of predictions from comparison preprocessing\n",
        "    comparisonname: name of comparison preprocessing\n",
        "    isChexModel: boolean to indicate if we are using chexmodel\n",
        "\n",
        "  Returns:\n",
        "    roc_df: dataframe with columns:\n",
        "      y_test_conditionName_basename representing true labels\n",
        "      preds_conditionName_basename representing predictions from base preprocessing\n",
        "      preds_conditionName_comparisonname representing predictions from comparison preprocessing\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #Create dataframe for export\n",
        "  roc_df = pd.DataFrame()\n",
        "\n",
        "  varsjpgIndex = conditionIndex\n",
        "\n",
        "  #If we are using chexmodel, ensure we have correct pathology matched up since it returns in a different order\n",
        "  if (isChexModel):\n",
        "    for x in range(len(included_pathologies)):\n",
        "      if (chex_pathologies[conditionIndex] == included_pathologies[x]):\n",
        "        varsjpgIndex = x\n",
        "  conditionName = included_pathologies[varsjpgIndex]\n",
        "\n",
        "\n",
        "  y_test = np.asarray(variables_jpg[1])[:,varsjpgIndex]\n",
        "  preds = np.asarray(base)[:,conditionIndex]\n",
        "\n",
        "  roc_df['y_test_' + conditionName + '_' + basename] = pd.Series(y_test)\n",
        "  roc_df['preds_' + conditionName + '_' + basename] = pd.Series(preds)\n",
        "\n",
        "  preds = np.asarray(comparison)[:,conditionIndex]\n",
        "\n",
        "  roc_df['preds_' + conditionName + '_' + comparisonname] = pd.Series(preds)\n",
        "\n",
        "  return roc_df\n",
        "\n",
        "def get_predictions_and_scatterpoints(model_results_name, dataset_results_name, conditionIndex, doCLAHEandJ2K, doCompRatio, savedVariables):\n",
        "\n",
        "  \"\"\"\n",
        "  Main function to obtain raw predictions and normalized predictions for scatterplots, organized into two dataframes.\n",
        "\n",
        "  Args:\n",
        "        model_results_name: name of model results to use, models = ['chexmodel', 'xrv_nih', 'xrv_mimic', 'xrv_chex']\n",
        "        dataset_results_name: name of dataset results to use, datasets = ['chex', 'mimic', 'NIH']\n",
        "        conditionIndex: index of condition to use, conditions = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
        "        doCLAHEandJ2K: boolean to indicate whether to include CLAHE and J2K in the results\n",
        "        doCompRatio: boolean to indicate whether to include compression ratios in the results\n",
        "        savedVariables: loaded prediction outputs from the previous results\n",
        "\n",
        "  Returns:\n",
        "        condition_df: dataframe containing the true labels, and raw predictions for each preprocessing type organized as columns\n",
        "        scatter_df: dataframe formatted to plot (x preds and y preds for each test). The x all represent preds from standard processing,\n",
        "    but are normalized relative to the minimum and maximum prediction values of that y pred preprocessing type.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  condition_df = pd.DataFrame()\n",
        "  scatter_df = pd.DataFrame()\n",
        "\n",
        "  if model_results_name == 'chexmodel':\n",
        "    condition = chex_pathologies[conditionIndex]\n",
        "    pathologies = chex_pathologies\n",
        "\n",
        "    chexModel = True\n",
        "  else:\n",
        "    condition = included_pathologies[conditionIndex]\n",
        "    pathologies = included_pathologies\n",
        "    chexModel = False\n",
        "\n",
        "\n",
        "  irNames = ['Bilinear',\n",
        "          'Interarea',\n",
        "          'Bilinear CV2',\n",
        "          'Cubic',\n",
        "          'Lanczos']\n",
        "\n",
        "  crNames = ['2', '4', '8', '16', '32']\n",
        "\n",
        "\n",
        "\n",
        "  #IR\n",
        "  #iterate through different types of resizing\n",
        "  for x in range(len(irNames)):\n",
        "\n",
        "    #Normalize standard and resizing data relative to the resizing prediction bounds\n",
        "    mi = min(min(savedVariables[1][condition].iloc[:,0]), min(savedVariables[1][condition].iloc[:,x]))\n",
        "    ma = max(max(savedVariables[1][condition].iloc[:,0]), max(savedVariables[1][condition].iloc[:,x]))\n",
        "\n",
        "    x_ax = (savedVariables[1][condition].iloc[:,0] - mi) / (ma - mi)\n",
        "    y_ax = (savedVariables[1][condition].iloc[:,x] - mi) / (ma - mi)\n",
        "\n",
        "    scatter_df[irNames[x] + '_x'] = x_ax\n",
        "    scatter_df[irNames[x] + '_y'] = y_ax\n",
        "\n",
        "    #On first iteration, create a new dataframe with results\n",
        "    if (condition_df.empty):\n",
        "\n",
        "      condition_df = pd.DataFrame(get_base_comparison_labels_and_preds(\n",
        "                            conditionIndex, vars_jpg,\n",
        "                            savedVariables[0][0],\n",
        "                            irNames[0],\n",
        "                            savedVariables[0][x],\n",
        "                            irNames[x],\n",
        "                            chexModel))\n",
        "\n",
        "    #Otherwise, concatenate the new predictions only to existing df\n",
        "    else:\n",
        "\n",
        "      nf = pd.DataFrame(get_base_comparison_labels_and_preds(conditionIndex, vars_jpg,\n",
        "                              savedVariables[0][0],\n",
        "                              irNames[0],\n",
        "                              savedVariables[0][x],\n",
        "                              irNames[x],\n",
        "                              chexModel))\n",
        "\n",
        "\n",
        "      condition_df = pd.concat([condition_df, nf.iloc[:,-1]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "  if doCLAHEandJ2K:\n",
        "\n",
        "    #Clahe\n",
        "    #zi = (xi – min(x)) / (max(x) – min(x))\n",
        "    mi = min(min(savedVariables[1][condition].iloc[:,0]), min(savedVariables[1][condition].iloc[:,5]))\n",
        "    ma = max(max(savedVariables[1][condition].iloc[:,0]), max(savedVariables[1][condition].iloc[:,5]))\n",
        "\n",
        "    x_ax = (savedVariables[1][condition].iloc[:,0] - mi) / (ma - mi)\n",
        "    y_ax = (savedVariables[1][condition].iloc[:,5] - mi) / (ma - mi)\n",
        "\n",
        "    scatter_df['CLAHE' + '_x'] = x_ax\n",
        "    scatter_df['CLAHE' + '_y'] = y_ax\n",
        "\n",
        "\n",
        "    nf = pd.DataFrame(get_base_comparison_labels_and_preds(conditionIndex, vars_jpg,\n",
        "                              savedVariables[0][0],\n",
        "                              irNames[0],\n",
        "                              savedVariables[0][5],\n",
        "                              'CLAHE TRUE',\n",
        "                              chexModel))\n",
        "    condition_df = pd.concat([condition_df, nf.iloc[:,-1]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    #J2k\n",
        "    mi = min(min(savedVariables[1][condition].iloc[:,0]), min(savedVariables[1][condition].iloc[:,6]))\n",
        "    ma = max(max(savedVariables[1][condition].iloc[:,0]), max(savedVariables[1][condition].iloc[:,6]))\n",
        "\n",
        "    x_ax = (savedVariables[1][condition].iloc[:,0] - mi) / (ma - mi)\n",
        "    y_ax = (savedVariables[1][condition].iloc[:,6] - mi) / (ma - mi)\n",
        "\n",
        "    scatter_df['J2K' + '_x'] = x_ax\n",
        "    scatter_df['J2K' + '_y'] = y_ax\n",
        "\n",
        "\n",
        "    nf = get_base_comparison_labels_and_preds(conditionIndex, vars_jpg,\n",
        "                              savedVariables[0][0],\n",
        "                              irNames[0],\n",
        "                              savedVariables[0][6],\n",
        "                              'J2K',\n",
        "                              chexModel)\n",
        "\n",
        "    condition_df = pd.concat([condition_df, nf.iloc[:,-1]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "  if doCompRatio:\n",
        "    for x in range(7, 12):\n",
        "\n",
        "      mi = min(min(savedVariables[1][condition].iloc[:,0]), min(savedVariables[1][condition].iloc[:,x]))\n",
        "      ma = max(max(savedVariables[1][condition].iloc[:,0]), max(savedVariables[1][condition].iloc[:,x]))\n",
        "\n",
        "      x_ax = (savedVariables[1][condition].iloc[:,0] - mi) / (ma - mi)\n",
        "      y_ax = (savedVariables[1][condition].iloc[:,x] - mi) / (ma - mi)\n",
        "\n",
        "      scatter_df[crNames[x-7] + '_x'] = x_ax\n",
        "      scatter_df[crNames[x-7] + '_y'] = y_ax\n",
        "\n",
        "      nf =  get_base_comparison_labels_and_preds(conditionIndex, vars_jpg,\n",
        "                              savedVariables[0][0],\n",
        "                              irNames[0],\n",
        "                              savedVariables[0][x],\n",
        "                              crNames[x-7],\n",
        "                              chexModel)\n",
        "      condition_df = pd.concat([condition_df, nf.iloc[:,-1]], axis=1)\n",
        "\n",
        "  return condition_df, scatter_df\n",
        "\n",
        "\n",
        "def loadModel(modelName):\n",
        "  if modelName == \"xrv_nih\":\n",
        "    model = xrv.models.DenseNet(weights=\"densenet121-res224-nih\", apply_sigmoid = False)\n",
        "  elif modelName == \"xrv_chex\":\n",
        "    model = xrv.models.DenseNet(weights=\"densenet121-res224-chex\", apply_sigmoid = False)\n",
        "  elif modelName == \"xrv_mimic\":\n",
        "    model = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_ch\", apply_sigmoid = False)\n",
        "  elif modelName == \"chexmodel\":\n",
        "    # Official Stanford CheXpert model\n",
        "    model = xrv.baseline_models.chexpert.DenseNet(weights_zip=contentFolder + 'chexpert_weights.zip', num_models=30)\n",
        "  return model\n",
        "\n",
        "def getYoudensThreshold(y_true, y_score):\n",
        "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true, y_score)\n",
        "    idx = np.argmax(tpr - fpr)\n",
        "    return thresholds[idx]\n",
        "\n",
        "def getCounts(df, thresholdOriginal):\n",
        "\n",
        "\n",
        "  df['res_Bilinear'] = \"\"\n",
        "\n",
        "\n",
        "  #Sensitivity = True positive / (true positive + false negatives)\n",
        "  #Specificity = True negative / (true negative + False positive)\n",
        "  tp_bl = 0\n",
        "  tn_bl = 0\n",
        "  fn_bl = 0\n",
        "  fp_bl = 0\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    if (df[df.columns[1]][i] < thresholdOriginal and df[df.columns[0]][i] == 0):\n",
        "      #True negative\n",
        "      tn_bl = tn_bl + 1\n",
        "      df['res_Bilinear'][i] = 'Correct'\n",
        "    elif (df[df.columns[1]][i] >= thresholdOriginal and df[df.columns[0]][i] == 1):\n",
        "      #True positive\n",
        "      tp_bl = tp_bl + 1\n",
        "      df['res_Bilinear'][i] = 'Correct'\n",
        "    elif (df[df.columns[1]][i] >= thresholdOriginal and df[df.columns[0]][i] == 0):\n",
        "      #Predicted as positive but result was negative\n",
        "      #False positive\n",
        "      fp_bl = fp_bl + 1\n",
        "      df['res_Bilinear'][i] = 'Incorrect'\n",
        "    elif (df[df.columns[1]][i] < thresholdOriginal and df[df.columns[0]][i] == 1):\n",
        "      #Predicted as negative but result was positive\n",
        "      #False negative\n",
        "      fn_bl = fn_bl + 1\n",
        "      df['res_Bilinear'][i] = 'Incorrect'\n",
        "\n",
        "  # Sensitivity\n",
        "  if (tp_bl + fn_bl) != 0:\n",
        "      sens_bl = tp_bl / (tp_bl + fn_bl)\n",
        "  else:\n",
        "      sens_bl = 0\n",
        "\n",
        "  # Specificity\n",
        "  if (tn_bl + fp_bl) != 0:\n",
        "      spec_bl = tn_bl / (tn_bl + fp_bl)\n",
        "  else:\n",
        "      spec_bl = 0\n",
        "\n",
        "  # Positive Predictive Value (PPV) or Precision\n",
        "  if (tp_bl + fp_bl) != 0:\n",
        "      ppv_bl = tp_bl / (tp_bl + fp_bl)\n",
        "  else:\n",
        "      ppv_bl = 0\n",
        "\n",
        "  # Negative Predictive Value (NPV)\n",
        "  if (tn_bl + fn_bl) != 0:\n",
        "      npv_bl = tn_bl / (tn_bl + fn_bl)\n",
        "  else:\n",
        "      npv_bl = 0\n",
        "\n",
        "  # Accuracy\n",
        "  total_bl = tp_bl + tn_bl + fp_bl + fn_bl\n",
        "  if total_bl != 0:\n",
        "      accuracy_bl = (tp_bl + tn_bl) / total_bl\n",
        "  else:\n",
        "      accuracy_bl = 0\n",
        "\n",
        "\n",
        "\n",
        "  res_names = [\"res_bilinear_full\", \"res_Interarea\", \"res_BilinearCV2\", \"res_Cubic\", \"res_Lanczos\", \"res_CLAHE\", \"res_J2K\", \"res_2\", \"res_4\", \"res_8\", \"res_16\", \"res_32\"]\n",
        "  pred_names = df.columns[1:]\n",
        "\n",
        "  tp_proc = [0] * len(res_names)\n",
        "  tn_proc = [0] * len(res_names)\n",
        "  fp_proc = [0] * len(res_names)\n",
        "  fn_proc = [0] * len(res_names)\n",
        "\n",
        "  AUROC_proc = [0] * len(res_names)\n",
        "\n",
        "\n",
        "  for i in range(len(res_names)):\n",
        "\n",
        "    #AUROC for each form of preprocessing\n",
        "    #auc = sklearn.metrics.roc_auc_score(np.asarray(labels)[:,i], np.asarray(outputs)[:,i])\n",
        "    y_test = df[df.columns[0]][:]\n",
        "    y_pred = df[df.columns[i+1]][:] #1 = bilinear, 2 = interarea...\n",
        "    AUROC_proc[i] = sklearn.metrics.roc_auc_score(np.asarray(y_test), np.asarray(y_pred))\n",
        "\n",
        "    #y_test = () #true labels\n",
        "    #print(df[df.columns[1]][:])\n",
        "\n",
        "    df[res_names[i]] = \"\"\n",
        "    for l in range(len(df)):\n",
        "      if( df[pred_names[i]][l] < thresholdOriginal and df[df.columns[0]][l] == 0 ):\n",
        "        #True negative\n",
        "        tn_proc[i] = tn_proc[i] + 1\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Consistently Correct'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Wrong Original, Correct Transformed'\n",
        "      elif( df[pred_names[i]][l] >= thresholdOriginal and df[df.columns[0]][l] == 1 ):\n",
        "        #True positive\n",
        "        tp_proc[i] = tp_proc[i] + 1\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Consistently Correct'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Wrong Original, Correct Transformed'\n",
        "      elif( df[pred_names[i]][l] >= thresholdOriginal and df[df.columns[0]][l] == 0 ):\n",
        "        #Predicted as positive, true answer was negative\n",
        "        #False positive\n",
        "        fp_proc[i] = fp_proc[i] + 1\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Correct Original, Wrong Transformed'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Consistently Wrong'\n",
        "      elif( df[pred_names[i]][l] < thresholdOriginal and df[df.columns[0]][l] == 1 ):\n",
        "        #Predicted as negative, true answer was positive\n",
        "        #False negative\n",
        "        fn_proc[i] = fn_proc[i] + 1\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Correct Original, Wrong Transformed'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Consistently Wrong'\n",
        "      else:\n",
        "        print(\"Something went wrong\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Convert the lists to numpy arrays for element-wise addition\n",
        "  tp_np = np.array(tp_proc)\n",
        "  tn_np = np.array(tn_proc)\n",
        "  fp_np = np.array(fp_proc)\n",
        "  fn_np = np.array(fn_proc)\n",
        "\n",
        "  # Sensitivity of preprocessed\n",
        "  sens_proc_num = tp_np\n",
        "  sens_proc_num = np.nan_to_num(sens_proc_num, nan=0)\n",
        "  sens_proc_denom = np.add(tp_np, fn_np)\n",
        "  sens_proc_denom = np.nan_to_num(sens_proc_denom, nan=0)\n",
        "  sens_proc = np.divide(sens_proc_num, sens_proc_denom)\n",
        "  sens_proc = np.nan_to_num(sens_proc, nan=0)\n",
        "  # Iterate through the elements of sens_proc_num and sens_proc_denom\n",
        "  sens_proc_frac = []\n",
        "  for num, denom in zip(sens_proc_num, sens_proc_denom):\n",
        "      # Construct the fraction string\n",
        "      fraction_str = f\"{num}/{denom}\"\n",
        "\n",
        "      # Append the fraction string to the sens_proc_frac list\n",
        "      sens_proc_frac.append(fraction_str)\n",
        "\n",
        "\n",
        "  # Specificity of preprocessed\n",
        "  spec_proc_num = tn_np\n",
        "  spec_proc_num = np.nan_to_num(spec_proc_num, nan=0)\n",
        "  spec_proc_denom = np.add(tn_np, fp_np)\n",
        "  spec_proc_denom = np.nan_to_num(spec_proc_denom, nan=0)\n",
        "  spec_proc = np.divide(spec_proc_num, spec_proc_denom)\n",
        "  spec_proc = np.nan_to_num(spec_proc, nan=0)\n",
        "\n",
        "  # Calculate Positive Predictive Value (PPV) or Precision\n",
        "  ppv_proc_num = tp_np\n",
        "  ppv_proc_num = np.nan_to_num(ppv_proc_num, nan=0)\n",
        "  ppv_proc_denom = np.add(tp_np, fp_np)\n",
        "  ppv_proc_denom = np.nan_to_num(ppv_proc_denom, nan=0)\n",
        "  ppv_proc = np.divide(ppv_proc_num, ppv_proc_denom)\n",
        "  ppv_proc = np.nan_to_num(ppv_proc, nan=0)\n",
        "\n",
        "\n",
        "  # Negative Predictive Value (NPV)\n",
        "  npv_proc_num = tn_np\n",
        "  npv_proc_num = np.nan_to_num(npv_proc_num, nan=0)\n",
        "  npv_proc_denom = np.add(tn_np, fn_np)\n",
        "  npv_proc_denom = np.nan_to_num(npv_proc_denom, nan=0)\n",
        "  npv_proc = np.divide(npv_proc_num, npv_proc_denom)\n",
        "  npv_proc = np.nan_to_num(npv_proc, nan=0)\n",
        "\n",
        "\n",
        "  # Accuracy\n",
        "  accuracy_proc_num = np.add(tp_np, tn_np)\n",
        "  accuracy_proc_num = np.nan_to_num(accuracy_proc_num, nan=0)\n",
        "  accuracy_proc_denom = np.add(np.add(tp_np, tn_np), np.add(fp_np, fn_np))\n",
        "  accuracy_proc_denom = np.nan_to_num(accuracy_proc_denom, nan=0)\n",
        "  accuracy_proc = np.divide(accuracy_proc_num, accuracy_proc_denom)\n",
        "  accuracy_proc = np.nan_to_num(accuracy_proc, nan=0)\n",
        "\n",
        "  percent_df = pd.DataFrame(columns = [\n",
        "\n",
        "    \"BaselineCorrect\",\n",
        "    \"Consistently Correct\",\n",
        "    \"Consistently Wrong\",\n",
        "    \"Correct Original, Wrong Transformed\",\n",
        "    \"Wrong Original, Correct Transformed\",\n",
        "    \"PercentFlipped\",\n",
        "    \"PreprocessedSensitivity\",\n",
        "    \"PreprocessedSpecificity\",\n",
        "    \"PreprocessedPPV\",\n",
        "    \"PreprocessedNPV\",\n",
        "    \"PreprocessedAccuracy\",\n",
        "    \"PreprocessedAUROC\"\n",
        "\n",
        "  ])\n",
        "  count_df = pd.DataFrame(columns = [\n",
        "\n",
        "    \"BaselineCorrect\",\n",
        "    \"Consistently Correct\",\n",
        "    \"Consistently Wrong\",\n",
        "    \"Correct Original, Wrong Transformed\",\n",
        "    \"Wrong Original, Correct Transformed\",\n",
        "    \"PercentFlipped\",\n",
        "    \"BaselineSensitivity\",\n",
        "    \"BaselineSpecificity\",\n",
        "    \"PreprocessedSensitivity\",\n",
        "    \"PreprocessedSpecificity\"\n",
        "  ])\n",
        "  #print(df)\n",
        "  for x in range(len(res_names)):\n",
        "    percent = (df[res_names[x]].value_counts(normalize=True) * 100)\n",
        "    percent = percent.rename(percent.name + \"_test\")\n",
        "    percent_df = percent_df.append(percent)\n",
        "    percent_df = percent_df.fillna(0)\n",
        "\n",
        "    count = df[res_names[x]].value_counts(normalize=False)\n",
        "    count = count.rename(count.name + \"_test\")\n",
        "    count_df = count_df.append(count)\n",
        "    count_df = count_df.fillna(0)\n",
        "\n",
        "\n",
        "  if('Correct Original, Wrong Transformed' in percent_df and 'Wrong Original, Correct Transformed' in percent_df):\n",
        "\n",
        "    percent_df['PercentFlipped'] = percent_df['Correct Original, Wrong Transformed'] + percent_df['Wrong Original, Correct Transformed']\n",
        "    percent_df['BaselineCorrect'] = percent_df['Consistently Correct'] + percent_df['Correct Original, Wrong Transformed']\n",
        "    percent_df['PreprocessedSensitivity'] = sens_proc\n",
        "    percent_df['PreprocessedSpecificity'] = spec_proc\n",
        "    percent_df[\"PreprocessedPPV\"] = ppv_proc\n",
        "    percent_df[\"PreprocessedNPV\"] = npv_proc\n",
        "    percent_df[\"PreprocessedAccuracy\"] = accuracy_proc\n",
        "    percent_df[\"PreprocessedAUROC\"] = AUROC_proc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    count_df['PercentFlipped'] = count_df['Correct Original, Wrong Transformed'] + count_df['Wrong Original, Correct Transformed']\n",
        "    count_df['BaselineCorrect'] = count_df['Consistently Correct'] + count_df['Correct Original, Wrong Transformed']\n",
        "    count_df['PreprocessedSensitivity_num'] = sens_proc_num\n",
        "    count_df['PreprocessedSensitivity_denom'] = sens_proc_denom\n",
        "    count_df['PreprocessedSpecificity_num'] = spec_proc_num\n",
        "    count_df['PreprocessedSpecificity_denom'] = spec_proc_denom\n",
        "    count_df['PreprocessedPPV_num'] = ppv_proc_num\n",
        "    count_df['PreprocessedPPV_denom'] = ppv_proc_denom\n",
        "    count_df['PreprocessedNPV_num'] = npv_proc_num\n",
        "    count_df['PreprocessedNPV_denom'] = npv_proc_denom\n",
        "    count_df['PreprocessedAccuracy_num'] = accuracy_proc_num\n",
        "    count_df['PreprocessedAccuracy_denom'] = accuracy_proc_denom\n",
        "\n",
        "\n",
        "    #count_df['PreprocessedSpecificity'] = spec_proc\n",
        "  return percent_df, count_df\n",
        "\n",
        "def getStatisticalResults(pathologies, savedVariables, variables_jpg):\n",
        "  data = []\n",
        "\n",
        "  for p in range(len(preprocessingMethods)):\n",
        "    for x in pathologies:\n",
        "      l = []\n",
        "      tval, pval = stats.ttest_rel(savedVariables[1][x].iloc[:,0], savedVariables[1][x].iloc[:,p])\n",
        "      l.append(f\"{preprocessingMethods[p]}_{x}\")\n",
        "      l.append(tval)\n",
        "      l.append(pval)\n",
        "      data.append(l)\n",
        "\n",
        "  df = pd.DataFrame(data, columns=['name', 'T_statistic', 'p_value'])\n",
        "  return df\n",
        "\n",
        "\n",
        "def getClassification(df, thresholdOriginal):\n",
        "\n",
        "  df['res_Bilinear'] = \"\"\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    if (df[df.columns[1]][i] < thresholdOriginal and df[df.columns[0]][i] == 0):\n",
        "      #True negative\n",
        "      df['res_Bilinear'][i] = 'Correct'\n",
        "    elif (df[df.columns[1]][i] >= thresholdOriginal and df[df.columns[0]][i] == 1):\n",
        "      #True positive\n",
        "      df['res_Bilinear'][i] = 'Correct'\n",
        "    elif (df[df.columns[1]][i] >= thresholdOriginal and df[df.columns[0]][i] == 0):\n",
        "      #Predicted as positive but result was negative\n",
        "      #False positive\n",
        "      df['res_Bilinear'][i] = 'Incorrect'\n",
        "    elif (df[df.columns[1]][i] < thresholdOriginal and df[df.columns[0]][i] == 1):\n",
        "      #Predicted as negative but result was positive\n",
        "      #False negative\n",
        "      df['res_Bilinear'][i] = 'Incorrect'\n",
        "\n",
        "\n",
        "  res_names = [\"res_Interarea\", \"res_BilinearCV2\", \"res_Cubic\", \"res_Lanczos\", \"res_CLAHE\", \"res_J2K\", \"res_2\", \"res_4\", \"res_8\", \"res_16\", \"res_32\"]\n",
        "  pred_names = df.columns[2:]\n",
        "\n",
        "  for i in range(len(res_names)):\n",
        "    df[res_names[i]] = \"\"\n",
        "    for l in range(len(df)):\n",
        "      if( df[pred_names[i]][l] <= thresholdOriginal and df[df.columns[0]][l] == 0 ):\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Consistently Correct'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Wrong Original, Correct Transformed'\n",
        "      elif( df[pred_names[i]][l] >= thresholdOriginal and df[df.columns[0]][l] == 1 ):\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Consistently Correct'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Wrong Original, Correct Transformed'\n",
        "      else:\n",
        "        #Transformed was incorrect\n",
        "        if(df['res_Bilinear'][l] == 'Correct'):\n",
        "          df[res_names[i]][l] = 'Correct Original, Wrong Transformed'\n",
        "        else:\n",
        "          df[res_names[i]][l] = 'Consistently Wrong'\n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Designated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN7WaofNghmb"
      },
      "outputs": [],
      "source": [
        "contentFolder = '/content/Compression/'\n",
        "\n",
        "destinationFolder = '/content/Compression/'\n",
        "\n",
        "#Models: \"chexmodel\" \"xrv_nih\" \"mimic_ch\" \"xrv_chex\"\n",
        "modelName = \"xrv_chex\"\n",
        "model = loadModel(modelName)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing By Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFzCS51GDXmR"
      },
      "outputs": [],
      "source": [
        "#TEST CHEXPERT DATA\n",
        "\n",
        "#LOAD LABELS AND IMAGE NAMES\n",
        "labelsDir = destinationFolder + 'chex_cohort/labels.csv'\n",
        "fileDir_jpg = destinationFolder + 'chex_cohort/'\n",
        "vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.jpg')\n",
        "\n",
        "fileDir_j2k = destinationFolder + 'chex_j2k/'\n",
        "fileDir_j2k_cr2 = destinationFolder + 'chex_j2k_cr_2/'\n",
        "fileDir_j2k_cr4 = destinationFolder + 'chex_j2k_cr_4/'\n",
        "fileDir_j2k_cr8 = destinationFolder + 'chex_j2k_cr_8/'\n",
        "fileDir_j2k_cr16 = destinationFolder + 'chex_j2k_cr_16/'\n",
        "fileDir_j2k_cr32 = destinationFolder + 'chex_j2k_cr32/'\n",
        "\n",
        "\n",
        "vars_j2k = organizeImages(fileDir_j2k, labelsDir, '.j2k')\n",
        "\n",
        "chex_ir_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_ir_bilinear_skimage\",\n",
        "          var = chex_ir_bilinear_skimage)\n",
        "chex_ir_interarea_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'interarea', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_ir_interarea_cv2\",\n",
        "          var = chex_ir_interarea_cv2)\n",
        "chex_ir_bilinear_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_ir_bilinear_cv2\",\n",
        "          var = chex_ir_bilinear_cv2)\n",
        "chex_ir_cubic_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'cubic', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_ir_cubic_cv2\",\n",
        "          var = chex_ir_cubic_cv2)\n",
        "chex_ir_lanczos_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'lanczos', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_ir_lanczos_cv2\",\n",
        "          var = chex_ir_lanczos_cv2)\n",
        "\n",
        "#Test CLAHE\n",
        "chex_clahe_true_bilinear_skimage = apply_model(model,\n",
        "                      clahe=True, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_clahe_true_bilinear_skimage\",\n",
        "            var = chex_clahe_true_bilinear_skimage)\n",
        "#Test Compression\n",
        "chex_compr_j2k_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_chex_compr_j2k_bilinear_skimage\",\n",
        "          var = chex_compr_j2k_bilinear_skimage)\n",
        "\n",
        "\n",
        "#Compression by compression ratio\n",
        "\n",
        "chex_compr_j2k_cr2_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr2, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"chex_compr_j2k_cr2_bilinear_skimage\",\n",
        "          var = chex_compr_j2k_cr2_bilinear_skimage)\n",
        "chex_compr_j2k_cr4_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr4, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"chex_compr_j2k_cr4_bilinear_skimage\",\n",
        "          var = chex_compr_j2k_cr4_bilinear_skimage)\n",
        "chex_compr_j2k_cr8_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr8, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"chex_compr_j2k_cr8_bilinear_skimage\",\n",
        "          var = chex_compr_j2k_cr8_bilinear_skimage)\n",
        "chex_compr_j2k_cr16_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr16, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"chex_compr_j2k_cr16_bilinear_skimage\",\n",
        "          var = chex_compr_j2k_cr16_bilinear_skimage)\n",
        "chex_compr_j2k_cr32_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr32, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"chex_compr_j2k_cr32_bilinear_skimage\",\n",
        "          var = chex_compr_j2k_cr32_bilinear_skimage)\n",
        "\n",
        "#TEST NIH DATA\n",
        "#LOAD LABELS AND IMAGE NAMES\n",
        "labelsDir = destinationFolder + 'NIH_cohort_balanced/labels.csv'\n",
        "fileDir_jpg = destinationFolder + 'NIH_cohort_balanced/'\n",
        "vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.png')\n",
        "\n",
        "fileDir_j2k = destinationFolder + 'NIH_J2K_balanced/'\n",
        "fileDir_j2k_cr2 = destinationFolder + 'NIH_j2k_cr_2/'\n",
        "fileDir_j2k_cr4 = destinationFolder + 'NIH_j2k_cr_4/'\n",
        "fileDir_j2k_cr8 = destinationFolder + 'NIH_j2k_cr_8/'\n",
        "fileDir_j2k_cr16 = destinationFolder + 'NIH_j2k_cr_16/'\n",
        "fileDir_j2k_cr32 = destinationFolder + 'NIH_j2k_cr32/'\n",
        "\n",
        "\n",
        "vars_j2k = organizeImages(fileDir_j2k, labelsDir, '.j2k')\n",
        "\n",
        "\n",
        "NIH_ir_bilinear_skimage = apply_model(model,\n",
        "                clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_ir_bilinear_skimage\",\n",
        "          var = NIH_ir_bilinear_skimage)\n",
        "NIH_ir_interarea_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'interarea', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_ir_interarea_cv2\",\n",
        "          var = NIH_ir_interarea_cv2)\n",
        "NIH_ir_bilinear_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_ir_bilinear_cv2\",\n",
        "          var = NIH_ir_bilinear_cv2)\n",
        "NIH_ir_cubic_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'cubic', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_ir_cubic_cv2\",\n",
        "          var = NIH_ir_cubic_cv2)\n",
        "NIH_ir_lanczos_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'lanczos', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path= contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_ir_lanczos_cv2\",\n",
        "            var = NIH_ir_lanczos_cv2)\n",
        "\n",
        "#Test CLAHE\n",
        "NIH_clahe_true_bilinear_skimage = apply_model(model,\n",
        "                      clahe=True, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_clahe_true_bilinear_skimage\",\n",
        "            var = NIH_clahe_true_bilinear_skimage)\n",
        "#Test Compression\n",
        "NIH_compr_j2k_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_NIH_compr_j2k_bilinear_skimage\",\n",
        "          var = NIH_compr_j2k_bilinear_skimage)\n",
        "\n",
        "#Compression by compression ratio\n",
        "\n",
        "NIH_compr_j2k_cr2_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr2, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"NIH_compr_j2k_cr2_bilinear_skimage\",\n",
        "          var = NIH_compr_j2k_cr2_bilinear_skimage)\n",
        "NIH_compr_j2k_cr4_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr4, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"NIH_compr_j2k_cr4_bilinear_skimage\",\n",
        "          var = NIH_compr_j2k_cr4_bilinear_skimage)\n",
        "NIH_compr_j2k_cr8_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr8, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"NIH_compr_j2k_cr8_bilinear_skimage\",\n",
        "          var = NIH_compr_j2k_cr8_bilinear_skimage)\n",
        "NIH_compr_j2k_cr16_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr16, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"NIH_compr_j2k_cr16_bilinear_skimage\",\n",
        "          var = NIH_compr_j2k_cr16_bilinear_skimage)\n",
        "NIH_compr_j2k_cr32_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr32, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"NIH_compr_j2k_cr32_bilinear_skimage\",\n",
        "          var = NIH_compr_j2k_cr32_bilinear_skimage)\n",
        "#TEST MIMIC DATA\n",
        "#LOAD LABELS AND IMAGE NAMES\n",
        "labelsDir = destinationFolder + 'mimic_cohort_png/labels.csv'\n",
        "fileDir_jpg = destinationFolder + 'mimic_cohort_png/'\n",
        "vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.png')\n",
        "\n",
        "fileDir_j2k = destinationFolder + 'mimic_j2k/'\n",
        "fileDir_j2k_cr2 = destinationFolder + 'mimic_j2k_cr2/'\n",
        "fileDir_j2k_cr4 = destinationFolder + 'mimic_j2k_cr4/'\n",
        "fileDir_j2k_cr8 = destinationFolder + 'mimic_j2k_cr8/'\n",
        "fileDir_j2k_cr16 = destinationFolder + 'mimic_j2k_cr16/'\n",
        "fileDir_j2k_cr32 = destinationFolder + 'mimic_j2k_cr32/'\n",
        "\n",
        "\n",
        "vars_j2k = organizeImages(fileDir_j2k, labelsDir, '.j2k')\n",
        "\n",
        "\n",
        "mimic_ir_bilinear_skimage = apply_model(model,\n",
        "                clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_ir_bilinear_skimage\",\n",
        "          var = mimic_ir_bilinear_skimage)\n",
        "mimic_ir_interarea_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'interarea', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_ir_interarea_cv2\",\n",
        "          var = mimic_ir_interarea_cv2)\n",
        "mimic_ir_bilinear_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_ir_bilinear_cv2\",\n",
        "          var = mimic_ir_bilinear_cv2)\n",
        "mimic_ir_cubic_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'cubic', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_ir_cubic_cv2\",\n",
        "          var = mimic_ir_cubic_cv2)\n",
        "mimic_ir_lanczos_cv2 = apply_model(model,\n",
        "                    clahe=False, resize = 'lanczos', eng = 'cv2', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path= contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_ir_lanczos_cv2\",\n",
        "            var = mimic_ir_lanczos_cv2)\n",
        "\n",
        "#Test CLAHE\n",
        "mimic_clahe_true_bilinear_skimage = apply_model(model,\n",
        "                      clahe=True, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_jpg, fileNames=vars_jpg[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_clahe_true_bilinear_skimage\",\n",
        "            var = mimic_clahe_true_bilinear_skimage)\n",
        "#Test Compression\n",
        "mimic_compr_j2k_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_mimic_compr_j2k_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_bilinear_skimage)\n",
        "\n",
        "#Compression by compression ratio\n",
        "\n",
        "mimic_compr_j2k_cr2_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr2, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr2_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr2_bilinear_skimage)\n",
        "mimic_compr_j2k_cr4_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr4, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr4_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr4_bilinear_skimage)\n",
        "mimic_compr_j2k_cr8_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr8, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr8_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr8_bilinear_skimage)\n",
        "mimic_compr_j2k_cr16_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr16, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr16_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr16_bilinear_skimage)\n",
        "mimic_compr_j2k_cr32_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr32, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr32_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr32_bilinear_skimage)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UAjwkuUDXYx"
      },
      "outputs": [],
      "source": [
        "#TEST MIMIC DATA\n",
        "#LOAD LABELS AND IMAGE NAMES\n",
        "labelsDir = destinationFolder + 'mimic_cohort_png/labels.csv'\n",
        "fileDir_jpg = destinationFolder + 'mimic_cohort_png/'\n",
        "vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.png')\n",
        "\n",
        "fileDir_j2k = destinationFolder + 'mimic_j2k/'\n",
        "fileDir_j2k_cr2 = destinationFolder + 'mimic_j2k_cr2/'\n",
        "fileDir_j2k_cr4 = destinationFolder + 'mimic_j2k_cr4/'\n",
        "fileDir_j2k_cr8 = destinationFolder + 'mimic_j2k_cr8/'\n",
        "fileDir_j2k_cr16 = destinationFolder + 'mimic_j2k_cr16/'\n",
        "fileDir_j2k_cr32 = destinationFolder + 'mimic_j2k_cr32/'\n",
        "\n",
        "\n",
        "vars_j2k = organizeImages(fileDir_j2k, labelsDir, '.j2k')\n",
        "\n",
        "\n",
        "#Compression by compression ratio\n",
        "\n",
        "mimic_compr_j2k_cr2_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr2, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr2_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr2_bilinear_skimage)\n",
        "mimic_compr_j2k_cr4_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr4, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr4_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr4_bilinear_skimage)\n",
        "mimic_compr_j2k_cr8_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr8, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr8_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr8_bilinear_skimage)\n",
        "mimic_compr_j2k_cr16_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr16, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr16_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr16_bilinear_skimage)\n",
        "mimic_compr_j2k_cr32_bilinear_skimage = apply_model(model,\n",
        "                    clahe=False, resize = 'bilinear', eng = 'skimage', fileDirectory=fileDir_j2k_cr32, fileNames=vars_j2k[0])\n",
        "pickleVar(path=contentFolder + \"/AutoSavedVariables/\"+modelName+\"_\" + \"mimic_compr_j2k_cr32_bilinear_skimage\",\n",
        "          var = mimic_compr_j2k_cr32_bilinear_skimage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9etOpNLahxk"
      },
      "source": [
        "RESULTS PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Models: \"chexmodel\" \"xrv_nih\" \"mimic_ch\"\n",
        "#Datasets: \"chex\" \"NIH\" \"mimic\"\n",
        "\n",
        "#models = ['chexmodel']\n",
        "#datasets = ['chex']\n",
        "#models = ['chexmodel', 'xrv_nih', 'xrv_mimic', 'xrv_chex']\n",
        "#datasets = ['chex', 'mimic', 'NIH']\n",
        "models = ['chexmodel', 'xrv_nih', 'xrv_mimic', 'xrv_chex']\n",
        "datasets = ['chex', 'mimic', 'NIH']\n",
        "\n",
        "#models = ['xrv_chex']\n",
        "#datasets = ['chex']\n",
        "\n",
        "res_names = [\"res_bilinear_full\", \"res_Interarea\", \"res_BilinearCV2\", \"res_Cubic\", \"res_Lanczos\", \"res_CLAHE\", \"res_J2K\", \"res_2\", \"res_4\", \"res_8\", \"res_16\", \"res_32\"]\n",
        "mean_residual_groups_named = [x + '_avg_residual' for x in res_names]\n",
        "perc_df_cols = ['BaselineCorrect',\t'Consistently Correct',\t'Consistently Wrong',\n",
        "                'Correct Original, Wrong Transformed',\t'Wrong Original, Correct Transformed',\n",
        "                \t'PercentFlipped',\t'PreprocessedSensitivity',\n",
        "                \t'PreprocessedSpecificity',\t'PreprocessedPPV',\t'PreprocessedNPV',\t'PreprocessedAccuracy',\n",
        "    'PreprocessedAUROC']\n",
        "count_df_cols = ['BaselineCorrect',\t'Consistently Correct',\t'Consistently Wrong',\n",
        "                'Correct Original, Wrong Transformed',\t'Wrong Original, Correct Transformed',\n",
        "                \t'PercentFlipped', 'BaselineSensitivity', 'BaselineSpecificity', 'PreprocessedSensitivity',\n",
        "                'PreprocessedSpecificity', 'PreprocessedSensitivity_num',\n",
        "                'PreprocessedSensitivity_denom', 'PreprocessedSpecificity_num',\n",
        "                'PreprocessedSpecificity_denom', 'PreprocessedPPV_num',\n",
        "                'PreprocessedPPV_denom', 'PreprocessedNPV_num', 'PreprocessedNPV_denom',\n",
        "                'PreprocessedAccuracy_num', 'PreprocessedAccuracy_denom']\n",
        "\n",
        "tstat_df = pd.DataFrame(columns = ['model', 'dataset', 'name', 'T_statistic', 'p_value'])\n",
        "\n",
        "tscatter_df = pd.DataFrame()\n",
        "tdeployed_list = []\n",
        "tcounts_list = []\n",
        "\n",
        "mean_residuals_sorted_list = []\n",
        "\n",
        "#Dataframe to hold all results data together\n",
        "columns_per_repetition_perc = len(perc_df_cols)\n",
        "columns_per_repetition_count = len(count_df_cols)\n",
        "\n",
        "\n",
        "# Define the prefixes for each repetition\n",
        "prefixes = res_names\n",
        "\n",
        "# Create a list of all column names (including the additional columns)\n",
        "all_columns_perc = ['model', 'dataset', 'disease_label'] + [f\"{prefix}_{perc_df_cols[i]}\" for prefix in prefixes for i in range(0, columns_per_repetition_perc)] + ['residual_order'] + mean_residual_groups_named\n",
        "all_columns_count = ['model', 'dataset', 'disease_label'] + [f\"{prefix}_{count_df_cols[i]}\" for prefix in prefixes for i in range(0, columns_per_repetition_count)]\n",
        "#Dfs for individually stratified results\n",
        "continuous_percent_df = pd.DataFrame(columns=all_columns_perc)\n",
        "continuous_counts_df = pd.DataFrame(columns=all_columns_count)\n",
        "\n",
        "for m in models:\n",
        "  if m == 'chexmodel':\n",
        "      pathologies = chex_pathologies\n",
        "  else:\n",
        "      pathologies = included_pathologies\n",
        "\n",
        "  for d in datasets:\n",
        "    mdscatter_df = pd.DataFrame()\n",
        "\n",
        "    if d == 'chex':\n",
        "      #TEST CHEX DATA\n",
        "      labelsDir = destinationFolder + 'chex_cohort/labels.csv'\n",
        "      fileDir_jpg = destinationFolder + 'chex_cohort/'\n",
        "      vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.jpg')\n",
        "    elif d == 'NIH':\n",
        "      #TEST NIH DATA\n",
        "      labelsDir = destinationFolder + 'NIH_cohort_balanced/labels.csv'\n",
        "      fileDir_jpg = destinationFolder + 'NIH_cohort_balanced/'\n",
        "      vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.png')\n",
        "\n",
        "    elif d == 'mimic':\n",
        "      #TEST MIMIC DATA\n",
        "      labelsDir = destinationFolder + 'mimic_cohort_png/labels.csv'\n",
        "      fileDir_jpg = destinationFolder + 'mimic_cohort_png/'\n",
        "      vars_jpg = organizeImages(fileDir_jpg, labelsDir, '.png')\n",
        "    else:\n",
        "      raise ValueError('Dataset name not recognized. Please use either \"chex\", \"NIH\", or \"mimic\".')\n",
        "\n",
        "    savedVariables = loadSavedVariables(pathologies, m, d, preprocessingMethods)\n",
        "    #If it is the NIH model and we want to exclude the points coded as RGB\n",
        "    if d == \"NIH\":\n",
        "      #Need RGB_indices defined\n",
        "      #fixing savedvariables[1]\n",
        "      for x in pathologies:\n",
        "        savedVariables[1][x] = savedVariables[1][x].drop(pd.Series(RGB_indices)).reset_index(drop=True)\n",
        "      #fixing savedvariables[0]\n",
        "      for x in range(len(savedVariables[0])):\n",
        "\n",
        "        for i in sorted(RGB_indices, reverse=True):\n",
        "          del savedVariables[0][x][i]\n",
        "      #Also fix vars_jpg\n",
        "      tmp_list = list(vars_jpg[1])\n",
        "      for i in sorted(RGB_indices, reverse=True):\n",
        "          del tmp_list[i]\n",
        "\n",
        "      tuple_list = list(vars_jpg)\n",
        "      # update item\n",
        "      tuple_list[1] = tmp_list\n",
        "      # convert list back to tuple\n",
        "      vars_jpg = tuple(tuple_list)\n",
        "\n",
        "    pa = getStatisticalResults(pathologies, savedVariables, vars_jpg)\n",
        "    pa.insert(0, 'dataset', d)\n",
        "    pa.insert(0, 'model', m)\n",
        "    tstat_df = pd.concat([tstat_df, pa], ignore_index=True, axis=0)\n",
        "\n",
        "\n",
        "    #This section is if we need to regenerate spreadsheets for a deployed model's performance\n",
        "    percent_df_list = []\n",
        "    count_df_list = []\n",
        "    regenerateSpreadsheets = True\n",
        "\n",
        "    if regenerateSpreadsheets:\n",
        "      for c in range(len(pathologies)):\n",
        "          #Skip consolidation since it's in chex model pathologies but we aren't using it\n",
        "          if (m == 'chexmodel') and c==2:\n",
        "            #no nothing\n",
        "            pass\n",
        "          else:\n",
        "            condition_df, points_df = get_predictions_and_scatterpoints(m, d, c, doCLAHEandJ2K = True, doCompRatio = True, savedVariables = savedVariables)\n",
        "\n",
        "            #Can calculate mean residuals here\n",
        "            #print(points_df.columns)\n",
        "\n",
        "            mean_residuals = []\n",
        "            mean_residual_groups = []\n",
        "            for x in range(0, len(points_df.columns), 2):\n",
        "\n",
        "\n",
        "              # Calculate the residuals from the line y = x\n",
        "              #print(points_df.columns[x+1])\n",
        "              #print(points_df.columns[x])\n",
        "              residuals = np.array(points_df[points_df.columns[x+1]]) - np.array(points_df[points_df.columns[x]])\n",
        "              abs_residuals = np.abs(residuals)\n",
        "\n",
        "              # Calculate the mean residual\n",
        "              mean_residual = np.mean(abs_residuals)\n",
        "\n",
        "              #print(\"Mean Residual:\")\n",
        "              #print(points_df.columns[x])\n",
        "              #print(mean_residual)\n",
        "\n",
        "\n",
        "\n",
        "              mean_residuals.append(mean_residual)\n",
        "              mean_residual_groups.append(points_df.columns[x])\n",
        "\n",
        "            # Sort the mean_residuals and mean_residual_groups together based on mean_residuals\n",
        "            sorted_mean_residuals, sorted_mean_residual_groups = zip(*sorted(zip(mean_residuals, mean_residual_groups)))\n",
        "\n",
        "            # Print mean_residual_groups in the sorted order\n",
        "            #print(\"Sorted Mean Residual Groups:\")\n",
        "            #print(sorted_mean_residual_groups)\n",
        "            mean_residuals_sorted_list.append(sorted_mean_residual_groups)\n",
        "\n",
        "            mdscatter_df = pd.concat([mdscatter_df, points_df], ignore_index=True)\n",
        "\n",
        "            conditionThreshold = getYoudensThreshold(condition_df[f'y_test_{pathologies[c]}_Bilinear'], condition_df[f'preds_{pathologies[c]}_Bilinear'])\n",
        "\n",
        "            percent_df, count_df = getCounts(condition_df, conditionThreshold)\n",
        "            if '' in count_df.columns:\n",
        "              count_df = count_df.drop('', axis=1)\n",
        "\n",
        "            #Adding to continuous_percent_df\n",
        "            percent_df_concat_list = percent_df.iloc[:, :len(perc_df_cols)].to_numpy().flatten().tolist()\n",
        "            new_row_perc_df = [m, d, pathologies[c]] + percent_df_concat_list + [sorted_mean_residual_groups] + mean_residuals\n",
        "\n",
        "            #Adding to continous_counts_df\n",
        "            counts_df_concat_list = count_df.iloc[:, :len(count_df_cols)].to_numpy().flatten().tolist()\n",
        "            new_row_count_df = [m, d, pathologies[c]] + counts_df_concat_list\n",
        "\n",
        "            #0 new_row_perc_df = [m, d, pathologies[c]] + percent_df_concat_list + [sorted_mean_residual_groups] + mean_residuals\n",
        "\n",
        "\n",
        "            print([m, d, pathologies[c]])\n",
        "\n",
        "            insert_index = len(continuous_percent_df)\n",
        "\n",
        "            # Insert the new row into continuous_percent_df\n",
        "            print(len(new_row_count_df))\n",
        "            print(len(all_columns_count))\n",
        "            continuous_percent_df.loc[insert_index] = new_row_perc_df\n",
        "            continuous_counts_df.loc[insert_index] = new_row_count_df\n",
        "\n",
        "            percent_df_list.append(percent_df)\n",
        "            count_df_list.append(count_df)\n",
        "\n",
        "          comb_percent_df = pd.concat(percent_df_list).groupby(level=0).mean()\n",
        "          comb_percent_df.to_csv(f\"/content/{m}_{d}_averaged.csv\")\n",
        "\n",
        "          comb_count_df = sum(count_df_list)\n",
        "          comb_count_df.to_csv(f\"/content/{m}_{d}_counts.csv\")\n",
        "\n",
        "\n",
        "\n",
        "    tscatter_df = pd.concat([tscatter_df, mdscatter_df], ignore_index=True)\n",
        "    tdeployed_list.append(comb_percent_df)\n",
        "    tcounts_list.append(comb_count_df)\n",
        "\n",
        "\n",
        "\n",
        "#tscatter_df is all set\n",
        "#tdeployed_list now needs to be averaged into a single dataframe\n",
        "tdeployed_df = pd.concat(tdeployed_list).groupby(level=0).mean()\n",
        "#tcounts_list needs to be summed into a single dataframe\n",
        "tcounts_df = sum(tcounts_list)\n",
        "#Write to csvs\n",
        "#tscatter contains the prediction outputs normalized\n",
        "tscatter_df.to_csv(\"/content/tscatter_df.csv\")\n",
        "#tdeployed contains the average values of correct/incorrect/sensitivity/specificity etc.\n",
        "tdeployed_df.to_csv(\"/content/tdeployed_df.csv\")\n",
        "#tcounts is used for chi squared tests of counts\n",
        "tcounts_df.to_csv(\"/content/tcounts_df.csv\")\n",
        "#continuous_percent_df contains percent_df which becomes tdeployed except\n",
        "#retains the individual results for each model x dataset x disease label\n",
        "continuous_percent_df.to_csv(\"/content/continuous_percent_df.csv\")\n",
        "\n",
        "continuous_counts_df.to_csv(\"/content/continuous_counts_df.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
